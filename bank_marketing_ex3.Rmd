---
title: "bank_marketing_ex3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Helper packages
library(dplyr)     # for data wrangling
library(ggplot2)   # for awesome plotting

# machine learning
library(ranger)   #<<  
library(rsample)  # data splitting
library(vip)      # visualize feature importance 
library(pdp)      # visualize feature effects
```

````{r}
#http://archive.ics.uci.edu/ml/datasets/Bank+Marketing

#read the data set
bank <- read.csv2("data/bank-additional-full.csv", dec = ".")
```

convert all characters to factors
```{r}
bank[sapply(bank, is.character)] <- lapply(bank[sapply(bank, is.character)], 
                                       as.factor)
```

drop duration and nr.employed columns 
```{r}
bank <- select(bank, c (-duration, - nr.employed))
```

```{r}
dim(bank)
names(bank)
head(bank)
```


take the 70% train dats set 

```{r}
set.seed(123) # for reproducibility
split <- initial_split(bank, strata = "y", prop = 0.7)
train <- training(split)
test  <- testing(split)

```

take smaller train set to check if the training method works well
```{r}
set.seed(123) # for reproducibility
split <- initial_split(bank, strata = "y", prop = 0.025)
train <- training(split)

```

MARS is not applicable for classification problems, therefore, we are going to use Random Forest

#Random Forest

```{r slide-50}
# number of features
features <- setdiff(names(train), "y")

# perform basic random forest model
fit_default <- ranger(
  formula    = y ~ ., 
  data       = train, 
  num.trees  = length(features) * 10,
  mtry       = floor(sqrt(length(features))),
  respect.unordered.factors = 'order',
  verbose    = FALSE,
  seed       = 123
  )

```

# Results 

Default results are based on OOB errors:

```{r slide-51}
# look at results
fit_default

```

```{r}
# compute RMSE (RMSE = square root of MSE)
sqrt(fit_default$prediction.error)
```


# Characteristics to Consider 

What we do next should be driven by attributes of our data:

- 8 variables are numeric
- 12 categorical variables with moderate number of levels


is the data unbalanced ??
feature engineering or design the tree 
without replacment

```{r slide-52a}
  train %>%
  summarise_if(is.factor, n_distinct) %>% 
  gather() %>% 
  arrange(desc(value))
```


- We have highly correlated data (both btwn features and with target)

Solution:
- May favor lower mtry and
- lower node size to help decorrelate the trees

OR
use PCA 
OR 
remove those columns with high correlation

```{r slide-52b}
cor_matrix <- train %>%
  mutate_if(is.factor, as.numeric) %>%
  cor()

# feature correlation
data_frame(
  row  = rownames(cor_matrix)[row(cor_matrix)[upper.tri(cor_matrix)]],
  col  = colnames(cor_matrix)[col(cor_matrix)[upper.tri(cor_matrix)]],
  corr = cor_matrix[upper.tri(cor_matrix)]
  ) %>%
  arrange(desc(abs(corr)))

# target correlation
data_frame(
    row  = rownames(cor_matrix)[row(cor_matrix)[upper.tri(cor_matrix)]],
    col  = colnames(cor_matrix)[col(cor_matrix)[upper.tri(cor_matrix)]],
    corr = cor_matrix[upper.tri(cor_matrix)]
) %>% filter(col == "y") %>%
    arrange(desc(abs(corr)))
```


# Tuning 

But before we tune, do we have enough trees

- Some pkgs provide OOB error for each tree
- __ranger__ only provides overall OOB
- from this chunk we can know the number of trees with the lowest number of RMSE

```{r slide-53a}
# number of features
n_features <- ncol(train) - 1

# ranger function
oob_error <- function(trees) {
  fit <- ranger(
  formula    = y ~ ., 
  data       = train, 
  num.trees  = trees, #<<
  mtry       = floor(sqrt(n_features)),
  respect.unordered.factors = 'order',
  verbose    = FALSE,
  seed       = 123
  )
  
  sqrt(fit$prediction.error)
}

# tuning grid
#begin with large 
trees <- seq(10, 1000, by = 20)

(rmse <- trees %>% purrr::map_dbl(oob_error))
```

arrange the data frame based on the lowest rmse
```{r}
data.frame(trees, rmse) %>% 
  arrange(rmse)
```

- the best number of trees is 970 trees with 0.3163265 rmse

- using $p \times 10 = 180$ trees is sufficient
- may increase if we decrease mtry or sample size

```{r slide-53b}
ggplot(data.frame(trees, rmse), aes(trees, rmse)) +
  geom_line(size = 1)
  # scale_x_continuous(breaks = trees)
```

# Tuning 

:Tuning grid

- lower end of mtry range due to correlation
- lower end of node size range due to correlation
- sampling w/o replacement due to categorical features 

```{r slide-54a}
hyper_grid <- expand.grid(
  mtry            = floor(n_features * c(.05, .15, .25, .333, .4)),
  min.node.size   = c(1, 3, 5),
  replace         = c(TRUE, FALSE),
  sample.fraction = c(.5, .63, .8),
  rmse            = NA
)

# number of hyperparameter combinations
nrow(hyper_grid)

head(hyper_grid)
```


Grid search execution

- This search grid took ~2.5 minutes
- __caret__ provides grid search 
- For larger data, use __H2O__'s random grid search with early stopping 

```{r slide-54b}
for(i in seq_len(nrow(hyper_grid))) {
  
  # fit model for ith hyperparameter combination
  fit <- ranger(
    formula         = y ~ ., 
    data            = train, 
    num.trees       = 970,
    mtry            = hyper_grid$mtry[i],            #<<
    min.node.size   = hyper_grid$min.node.size[i],   #<<
    replace         = hyper_grid$replace[i],         #<<
    sample.fraction = hyper_grid$sample.fraction[i], #<<
    verbose         = FALSE,
    seed            = 123,
    respect.unordered.factors = 'order',
  )
  
  # export OOB error 
  hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
  
}
```

# Tuning results 

Our top 10 models:

- have ~1% or higher performance improvement over the default model????????
- sample w/o replacement
- primarily include higher sampling????????
- primarily use mtry = 20 or 26??????
- node size appears non-influential???????

I would follow this up with an additional grid search that focuses on:

- mtry values around 0,2,4,7
- sample fraction around 50%, 63%

_using too high of sampling fraction without replacement runs the risk of overfitting to your training data!_

```{r slide-55}
default_rmse <- sqrt(fit_default$prediction.error)

hyper_grid %>%
  arrange(rmse) %>%
  mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
  head(10)
```


# Feature Importance 

Once you find your optimal model:

- re-run with the respective hyperparameters
- include `importance` parameter
- crank up the # of trees to ensure stable vi estimates

```{r slide-58}
fit_final <- ranger(
  formula         = y ~ ., 
  data            = train, 
  num.trees       = 970,              #<<
  mtry            = 2,
  min.node.size   = 5,
  sample.fraction = .80,
  replace         = TRUE,
  importance      = 'permutation',     #<< 
  respect.unordered.factors = 'order',
  verbose         = FALSE,
  seed            = 123,
  probability = TRUE
  )

vip(fit_final, num_features = 15)
```

# Feature Effects 

Partial dependence plots (PDPs), Individual Conditional Expectation (ICE) curves, and other approaches allow us to understand how _important_ variables influence our model's predictions:

PDP: emp.var.rate

```{r slide-59a}
fit_final %>%
  pdp::partial(pred.var = "emp.var.rate", train = as.data.frame(train)) %>%
  autoplot()
```

ICE: emp.var.rate

```{r slide-59b}
fit_final %>%
  pdp::partial(pred.var = "emp.var.rate", train = as.data.frame(train), ice = TRUE) %>%
  autoplot(alpha = 0.05, center = TRUE)
```

PDP: Above Ground SqFt

```{r slide-60a}
fit_final %>%
  pdp::partial(pred.var = "euribor3m", train = as.data.frame(train)) %>%
  autoplot()
```

ICE: Above Ground SqFt

```{r slide-60b}
fit_final %>%
  pdp::partial(pred.var = "euribor3m", train = as.data.frame(train), ice = TRUE) %>%
  autoplot(alpha = 0.05, center = TRUE)
```


Interaction between two influential variables:
what is the point from this ?????? yes 

```{r slide-61}
fit_final %>%
  pdp::partial(
    pred.var = c("emp.var.rate", "euribor3m"),
    train = as.data.frame(train)
    ) %>%
  plotPartial(
    zlab = "y",
    levelplot = FALSE, 
    drape = TRUE, 
    colorkey = FALSE,
    screen = list(z = 50, x = -60)
  )
```

