---
title: "bank_marketing_ex4_2"
author: "*Buthaina Alshareef*"
date: "*01-12-2020*"
output:
  html_document:
   toc: true
   toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




```{r}
library(recipes)
library(h2o)
# Packages required
library(rsample)
library(caret)
library(tidyverse)
library(inspectdf) 
library(ISLR)
library(dplyr)
library(ggplot2)
library(psych)

```

```{r}
h2o.init(max_mem_size = "5g")
```





```{r}
#http://archive.ics.uci.edu/ml/datasets/Bank+Marketing

#read the data set
bank <- read.csv2("data/bank-additional-full.csv", dec = ".")
```

convert all characters to factors
```{r}
bank[sapply(bank, is.character)] <- lapply(bank[sapply(bank, is.character)], 
                                           as.factor)
```

drop duration and nr.employed columns 
```{r}
bank <- select(bank, c (-duration, - nr.employed))
```

take 70% train set to check if the training method works well
```{r}
set.seed(123) # for reproducibility
split <- initial_split(bank, strata = "y", prop = 0.7)
train <- training(split)
test <- testing(split)

```



```{r}

# make sure we have consistent categorical levels
blueprint <- recipe(y ~ ., data = train) %>%
  step_other(all_nominal(), threshold = .005)

# create training & test sets
train_h2o <- prep(blueprint, training = train, retain = TRUE) %>%
  juice() %>%
  as.h2o()

test_h2o <- prep(blueprint, training = train) %>%
  bake(new_data = test) %>%
  as.h2o()

# get names of response and features
Y <- "y"
X <- setdiff(names(train), Y)

```


#6

Using H2O, build and assess the following individual models:
- random forest base learner.
- GBM and/or XGBoost base learner.


```{r}
best_rf <- h2o.randomForest(
  x = X,
  y = Y,
  training_frame = train_h2o, 
  ntree = 100, 
  mtries = 2,
  nbins = 50,
  sample_rate = 0.95,
  keep_cross_validation_predictions = T,
  binomial_double_trees = F,
  fold_assignment = "Modulo",
  nfolds = 10, 
  min_rows = 200,
  balance_classes = T,
  max_depth = 12)


best_rf

#best_rf@model$training_metrics@metrics$Gini

```


```{r slide-133}
# Train & Cross-validate a GBM model
best_gbm <- h2o.gbm(
  x = X,
  y = Y,
  training_frame = train_h2o,
  ntrees = 5000,
  learn_rate = 0.01,
  max_depth = 7,
  min_rows = 5,
  sample_rate = 0.8,
  nfolds = 10, 
  fold_assignment = "Modulo", 
  keep_cross_validation_predictions = TRUE, 
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "AUTO",
  stopping_tolerance = 0,
  categorical_encoding = "Binary"
)

best_gbm
```


```{r slide-133}
# to save time I will ignore this

# Train & Cross-validate an XGBoost model
# best_xgb <- h2o.xgboost(
#   x = X,
#   y = Y,
#   training_frame = train_h2o,
#   ntrees = 5000,
#   learn_rate = 0.05,
#   max_depth = 3,
#   min_rows = 3,
#   sample_rate = 0.8,
#   categorical_encoding = "Binary",
#   nfolds = 10,
#   fold_assignment = "Modulo",
#   keep_cross_validation_predictions = TRUE,
#   seed = 123,
#   stopping_rounds = 50,
#   stopping_metric = "AUTO",
#   stopping_tolerance = 0
# )
# 
# #h2o.rmse(best_xgb, xval = TRUE)
# best_xgb
```


#7
Using h2o.stackedEnsemble(), stack these three models.
- Does your stacked model performance improve over and above the individual learners?
- Explain your reasoning why or why not performance improves.


```{r slide-13}
ensemble_tree <- h2o.stackedEnsemble(
  x = X,
  y = Y,
  training_frame = train_h2o,
  model_id = "my_tree_ensemble",
  base_models = list(best_rf, best_gbm),
  metalearner_algorithm = "gbm"
)
ensemble_tree
```




```{r slide-14a}
per_rf <- h2o.performance(best_rf, newdata = test_h2o)
  arrange(h2o.accuracy(per_rf))
```


```{r}
per_gbm <- h2o.performance(best_gbm, newdata = test_h2o)
  arrange(h2o.accuracy(per_gbm))
```


```{r }
# per_xgb <- h2o.performance(best_xgb, newdata = test_h2o)
#   arrange(h2o.accuracy(per_xgb))
```


```{r }

# stacked glm
results_tree <- h2o.performance(ensemble_tree, newdata = test_h2o)
  arrange(h2o.accuracy(results_tree))
```




#7
Perform a stacked grid search with an H2O GBM or XGBoost model.
- What was your best performing model?
- Do you notice any patterns in the hyperparameter settings for the top 5-10 models?

```{r slide-16}
# GBM hyperparameters
hyper_grid <- list(
  max_depth = c(1, 3, 5),
  min_rows = c(1, 5, 10),
  learn_rate = c(0.01, 0.05, 0.1),
  learn_rate_annealing = c(.99, 1),
  sample_rate = c(.5, .75, 1),
  col_sample_rate = c(.8, .9, 1)
)
```


```{r slide-16}
# random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  max_models = 25
)
```


```{r slide-16}
# build random grid search 
random_grid <- h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid",
  x = X,
  y = Y,
  training_frame = train_h2o,
  hyper_params = hyper_grid,
  search_criteria = search_criteria,
  ntrees = 5000,
  stopping_metric = "AUTO",
  stopping_rounds = 10,
  stopping_tolerance = 0,
  nfolds = 10,
  fold_assignment = "Modulo",
  keep_cross_validation_predictions = TRUE,
  seed = 123
)
```


```{r slide-16}
# collect the results and sort by our model performance metric of choice
random_grid_perf <- h2o.getGrid(
  grid_id = "gbm_grid", 
  sort_by = "accuracy"
)
random_grid_perf
```

Single best model applied to our test set:

```{r slide-18a}
# Grab the model_id for the top model, chosen by validation error
best_model_id <- random_grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
h2o.performance(best_model, newdata = test_h2o)
```

Meta learner of our grid search applied to our test set:

```{r slide-18b}
# Train a stacked ensemble using the GBM grid
ensemble <- h2o.stackedEnsemble(
  x = X,
  y = Y,
  training_frame = train_h2o,
  model_id = "ensemble_gbm_grid",
  base_models = random_grid@model_ids,
  metalearner_algorithm = "gbm"
)

# Eval ensemble performance on a test set
h2o.performance(ensemble, newdata = test_h2o)
```


#8
Perform an AutoML search across multiple types of learners.


```{r slide-21}
auto_ml <- h2o.automl(
  x = X,
  y = Y,
  training_frame = train_h2o,
  nfolds = 5,
  max_runtime_secs = 60*120, # 2 hours!
  keep_cross_validation_predictions = TRUE,
  sort_metric = "AUC",
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "misclassification",
  stopping_tolerance = 0
)

# assess the leader board
# get top model: auto_ml@leader
auto_ml@leaderboard %>% as.data.frame()
```




- Which types of base learners are in the top 10?
- What model provides the optimal performance?
- Apply this model to the test set. How does the test loss function compare to the training cross-validated RMSE?


```{r}
save.image("myWorkspace_ex4_22.rds")
```


```{r}
load("myWorkspace_ex4_22.rds")
```
