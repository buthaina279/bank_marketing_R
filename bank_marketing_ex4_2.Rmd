---
title: "bank_marketing_ex4_2"
author: "*Buthaina Alshareef*"
date: "*01-12-2020*"
output:
  html_document:
   toc: true
   toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




```{r, include=FALSE}
library(recipes)
library(h2o)
# Packages required
library(rsample)
library(caret)
library(tidyverse)
library(inspectdf) 
library(ISLR)
library(dplyr)
library(ggplot2)
library(psych)

```

```{r, cache=TRUE, include=FALSE }
h2o.init(max_mem_size = "5g")
```





```{r, cache=TRUE}
#http://archive.ics.uci.edu/ml/datasets/Bank+Marketing

#read the data set
bank <- read.csv2("data/bank-additional-full.csv", dec = ".")
```

convert all characters to factors
```{r, cache=TRUE}
bank[sapply(bank, is.character)] <- lapply(bank[sapply(bank, is.character)], 
                                           as.factor)
```

drop duration and nr.employed columns 
```{r, cache=TRUE}
bank <- select(bank, c (-duration, - nr.employed))
```

take 70% train set to check if the training method works well
```{r, cache=TRUE}
set.seed(123) # for reproducibility
split <- initial_split(bank, strata = "y", prop = 0.7)
train <- training(split)
test <- testing(split)

```



```{r, cache=TRUE}

# make sure we have consistent categorical levels
blueprint <- recipe(y ~ ., data = train) %>%
  step_other(all_nominal(), threshold = .005)

# create training & test sets
train_h2o <- prep(blueprint, training = train, retain = TRUE) %>%
  juice() %>%
  as.h2o()

test_h2o <- prep(blueprint, training = train) %>%
  bake(new_data = test) %>%
  as.h2o()

# get names of response and features
Y <- "y"
X <- setdiff(names(train), Y)

```


#6

Using H2O, build and assess the following individual models:
- random forest base learner.
- GBM and/or XGBoost base learner.

the error rate in Random Forest is higher than gbm based on MSE with 0.07789691 and 0.0770109 respectively



```{r, cache=TRUE}
best_rf <- h2o.randomForest(
  x = X,
  y = Y,
  training_frame = train_h2o, 
  ntree = 100, 
  mtries = 2,
  nbins = 50,
  sample_rate = 0.95,
  keep_cross_validation_predictions = T,
  binomial_double_trees = F,
  fold_assignment = "Modulo",
  nfolds = 10, 
  min_rows = 200,
  balance_classes = T,
  max_depth = 12)


best_rf

#best_rf@model$training_metrics@metrics$Gini

```


```{r slide-133, cache=TRUE}
# Train & Cross-validate a GBM model
best_gbm <- h2o.gbm(
  x = X,
  y = Y,
  training_frame = train_h2o,
  ntrees = 5000,
  learn_rate = 0.01,
  max_depth = 7,
  min_rows = 5,
  sample_rate = 0.8,
  nfolds = 10, 
  fold_assignment = "Modulo", 
  keep_cross_validation_predictions = TRUE, 
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "AUTO",
  stopping_tolerance = 0,
  categorical_encoding = "Binary"
)

best_gbm
```


```{r, , cache=TRUE, echo= FALSE, message= FALSE}
# to save time I will ignore this

# Train & Cross-validate an XGBoost model
# best_xgb <- h2o.xgboost(
#   x = X,
#   y = Y,
#   training_frame = train_h2o,
#   ntrees = 5000,
#   learn_rate = 0.05,
#   max_depth = 3,
#   min_rows = 3,
#   sample_rate = 0.8,
#   categorical_encoding = "Binary",
#   nfolds = 10,
#   fold_assignment = "Modulo",
#   keep_cross_validation_predictions = TRUE,
#   seed = 123,
#   stopping_rounds = 50,
#   stopping_metric = "AUTO",
#   stopping_tolerance = 0
# )
# 
# #h2o.rmse(best_xgb, xval = TRUE)
# best_xgb
```


#7
Using h2o.stackedEnsemble(), stack these three models.
- Does your stacked model performance improve over and above the individual learners?
yes it becomes 0.06649911 MSE 


```{r slide-13, cache=TRUE}
ensemble_tree <- h2o.stackedEnsemble(
  x = X,
  y = Y,
  training_frame = train_h2o,
  model_id = "my_tree_ensemble",
  base_models = list(best_rf, best_gbm),
  metalearner_algorithm = "gbm"
)
ensemble_tree
```




```{r slide-14a, cache=TRUE}
per_rf <- h2o.performance(best_rf, newdata = test_h2o)
  arrange(h2o.accuracy(per_rf))
```


```{r, cache=TRUE}
per_gbm <- h2o.performance(best_gbm, newdata = test_h2o)
  arrange(h2o.accuracy(per_gbm))
```


```{r , cache=TRUE}
# per_xgb <- h2o.performance(best_xgb, newdata = test_h2o)
#   arrange(h2o.accuracy(per_xgb))
```


```{r , cache=TRUE}

# stacked glm
results_tree <- h2o.performance(ensemble_tree, newdata = test_h2o)
  arrange(h2o.accuracy(results_tree))
```




#7
Perform a stacked grid search with an H2O GBM or XGBoost model.

Does not work
```{r , cache=TRUE}
# GBM hyperparameters
hyper_grid <- list(
  max_depth = c(1, 3, 5),
  min_rows = c(1, 5, 10),
  learn_rate = c(0.01, 0.05, 0.1),
  learn_rate_annealing = c(.99, 1),
  sample_rate = c(.5, .75, 1),
  col_sample_rate = c(.8, .9, 1)
)
```


```{r , cache=TRUE}
# random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  max_models = 25
)
```


```{r , cache=TRUE}
# build random grid search 
# random_grid <- h2o.grid(
#   algorithm = "gbm",
#   grid_id = "gbm_grid",
#   x = X,
#   y = Y,
#   training_frame = train_h2o,
#   hyper_params = hyper_grid,
#   search_criteria = search_criteria,
#   ntrees = 5000,
#   stopping_metric = "AUTO",
#   stopping_rounds = 10,
#   stopping_tolerance = 0,
#   nfolds = 10,
#   fold_assignment = "Modulo",
#   keep_cross_validation_predictions = TRUE,
#   seed = 123
# )
```


```{r , cache=TRUE}
# random_grid_perf <- h2o.getGrid(
#   grid_id = "gbm_grid",
#   sort_by = "accuracy"
# )
# random_grid_perf
```

Single best model applied to our test set:

```{r ,cache=TRUE}
# Grab the model_id for the top model, chosen by validation error
# best_model_id <- random_grid_perf@model_ids[[1]]
# best_model <- h2o.getModel(best_model_id)
# h2o.performance(best_model, newdata = test_h2o)
```

Meta learner of our grid search applied to our test set:

```{r}
# Train a stacked ensemble using the GBM grid
# ensemble <- h2o.stackedEnsemble(
#   x = X,
#   y = Y,
#   training_frame = train_h2o,
#   model_id = "ensemble_gbm_grid",
#   base_models = random_grid@model_ids,
#   metalearner_algorithm = "gbm"
# )
# 
# # Eval ensemble performance on a test set
# h2o.performance(ensemble, newdata = test_h2o)
```


#8
Perform an AutoML search across multiple types of learners.


```{r, cache=TRUE}
# auto_ml <- h2o.automl(
#   x = X,
#   y = Y,
#   training_frame = train_h2o,
#   nfolds = 5,
#   max_runtime_secs = 60*120, # 2 hours!
#   keep_cross_validation_predictions = TRUE,
#   sort_metric = "AUC",
#   seed = 123,
#   stopping_rounds = 50,
#   stopping_metric = "misclassification",
#   stopping_tolerance = 0
# )
# 
# # assess the leader board
# # get top model: auto_ml@leader
# auto_ml@leaderboard %>% as.data.frame()
```



```{r}
#save.image("myWorkspace_ex4_22.rds")
```


```{r}
#load("myWorkspace_ex4_22.rds")
```
