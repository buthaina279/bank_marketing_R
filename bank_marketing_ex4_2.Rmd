---
title: "bank_marketing_ex4_2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




```{r}
library(recipes)
library(h2o)
```

```{r}
h2o.init(max_mem_size = "5g")
```





```{r}
#http://archive.ics.uci.edu/ml/datasets/Bank+Marketing

#read the data set
bank <- read.csv2("data/bank-additional-full.csv", dec = ".")
```

convert all characters to factors
```{r}
bank[sapply(bank, is.character)] <- lapply(bank[sapply(bank, is.character)], 
                                       as.factor)
```

drop duration and nr.employed columns 
```{r}
bank <- select(bank, c (-duration, - nr.employed))
```

take smaller train set to check if the training method works well
```{r}
set.seed(123) # for reproducibility
split <- initial_split(bank, strata = "y", prop = 0.025)
train <- training(split)
test <- testing(split)

```



```{r}

# make sure we have consistent categorical levels
blueprint <- recipe(y ~ ., data = train) %>%
  step_other(all_nominal(), threshold = .005)

# create training & test sets
train_h2o <- prep(blueprint, training = train, retain = TRUE) %>%
  juice() %>%
  as.h2o()

test_h2o <- prep(blueprint, training = train) %>%
  bake(new_data = test) %>%
  as.h2o()

# get names of response and features
Y <- "y"
X <- setdiff(names(train), Y)

```


#6

Using H2O, build and assess the following individual models:
- regularized regression base learner,
- random forest base learner.
- GBM and/or XGBoost base learner.

```{r slide-12}
# Train & Cross-validate a GLM model
best_glm <- h2o.glm(
  x = X,
  y = Y,
  training_frame = train_h2o,
  alpha = .1,
  remove_collinear_columns = TRUE,
  nfolds = 10, 
  fold_assignment = "Modulo", 
  keep_cross_validation_predictions = TRUE, 
  seed = 123
  )

h2o.rmse(best_glm, xval = TRUE)

# Train & Cross-validate a RF model
best_rf <- h2o.randomForest(
  x = X,
  y = Y,
  training_frame = train_h2o,
  ntrees = 1000,
  mtries = 20,
  max_depth = 30,
  min_rows = 1,
  sample_rate = 0.8,
  nfolds = 10, 
  fold_assignment = "Modulo", 
  keep_cross_validation_predictions = TRUE, 
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "RMSE",
  stopping_tolerance = 0
  )

h2o.rmse(best_rf, xval = TRUE)

# Train & Cross-validate a GBM model
best_gbm <- h2o.gbm(
  x = X,
  y = Y,
  training_frame = train_h2o,
  ntrees = 5000,
  learn_rate = 0.01,
  max_depth = 7,
  min_rows = 5,
  sample_rate = 0.8,
  nfolds = 10, 
  fold_assignment = "Modulo", 
  keep_cross_validation_predictions = TRUE, 
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "RMSE",
  stopping_tolerance = 0
  )

h2o.rmse(best_gbm, xval = TRUE)

# Train & Cross-validate an XGBoost model
best_xgb <- h2o.xgboost(
  x = X,
  y = Y,
  training_frame = train_h2o,
  ntrees = 5000,
  learn_rate = 0.05,
  max_depth = 3,
  min_rows = 3,
  sample_rate = 0.8,
  categorical_encoding = "Enum",
  nfolds = 10,
  fold_assignment = "Modulo", 
  keep_cross_validation_predictions = TRUE, 
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "RMSE",
  stopping_tolerance = 0
)

h2o.rmse(best_xgb, xval = TRUE)
```


#7
Using h2o.stackedEnsemble(), stack these three models.
- Does your stacked model performance improve over and above the individual learners?
- Explain your reasoning why or why not performance improves.


```{r slide-13}
ensemble_tree <- h2o.stackedEnsemble(
  x = X,
  y = Y,
  training_frame = train_h2o,
  model_id = "my_tree_ensemble",
  base_models = list(best_glm, best_rf, best_gbm, best_xgb),
  metalearner_algorithm = "drf"
  )
```

```{r slide-14a}
# base learners
get_rmse <- function(model) {
  results <- h2o.performance(model, newdata = test_h2o)
  results@metrics$RMSE
}
list(best_glm, best_rf, best_gbm, best_xgb) %>%
  purrr::map_dbl(get_rmse)

# stacked glm
results_tree <- h2o.performance(ensemble_tree, newdata = test_h2o)
results_tree@metrics$RMSE
```


We're restricted on how much improvement stacking will make due to highly correlated predictions

```{r slide-14b}
data.frame(
  GLM_pred = as.vector(h2o.getFrame(best_glm@model$cross_validation_holdout_predictions_frame_id$name)),
  RF_pred = as.vector(h2o.getFrame(best_rf@model$cross_validation_holdout_predictions_frame_id$name)),
  GBM_pred = as.vector(h2o.getFrame(best_gbm@model$cross_validation_holdout_predictions_frame_id$name)),
  XGB_pred = as.vector(h2o.getFrame(best_xgb@model$cross_validation_holdout_predictions_frame_id$name))
  ) %>%
  cor()
```

#7
Perform a stacked grid search with an H2O GBM or XGBoost model.
- What was your best performing model?
- Do you notice any patterns in the hyperparameter settings for the top 5-10 models?

```{r slide-16}
# GBM hyperparameters
hyper_grid <- list(
  max_depth = c(1, 3, 5),
  min_rows = c(1, 5, 10),
  learn_rate = c(0.01, 0.05, 0.1),
  learn_rate_annealing = c(.99, 1),
  sample_rate = c(.5, .75, 1),
  col_sample_rate = c(.8, .9, 1)
)

# random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  max_models = 25
  )

# build random grid search 
random_grid <- h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid",
  x = X,
  y = Y,
  training_frame = train_h2o,
  hyper_params = hyper_grid,
  search_criteria = search_criteria,
  ntrees = 5000,
  stopping_metric = "RMSE",     
  stopping_rounds = 10,         
  stopping_tolerance = 0,
  nfolds = 10,
  fold_assignment = "Modulo",
  keep_cross_validation_predictions = TRUE,
  seed = 123
  )

# collect the results and sort by our model performance metric of choice
random_grid_perf <- h2o.getGrid(
  grid_id = "gbm_grid", 
  sort_by = "rmse"
  )
random_grid_perf
```

Single best model applied to our test set:

```{r slide-18a}
# Grab the model_id for the top model, chosen by validation error
best_model_id <- random_grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
h2o.performance(best_model, newdata = test_h2o)
```

Meta learner of our grid search applied to our test set:

```{r slide-18b}
# Train a stacked ensemble using the GBM grid
ensemble <- h2o.stackedEnsemble(
  x = X,
  y = Y,
  training_frame = train_h2o,
  model_id = "ensemble_gbm_grid",
  base_models = random_grid@model_ids,
  metalearner_algorithm = "gbm"
  )

# Eval ensemble performance on a test set
h2o.performance(ensemble, newdata = test_h2o)
```


#8
Perform an AutoML search across multiple types of learners.

   
```{r slide-21}
auto_ml <- h2o.automl(
  x = X,
  y = Y,
  training_frame = train_h2o,
  nfolds = 5,
  max_runtime_secs = 60*120, # 2 hours!
  keep_cross_validation_predictions = TRUE,
  sort_metric = "RMSE",
  seed = 123,
  stopping_rounds = 50,
  stopping_metric = "RMSE",
  stopping_tolerance = 0
)

# assess the leader board
# get top model: auto_ml@leader
auto_ml@leaderboard %>% as.data.frame()
```
   
   
- Which types of base learners are in the top 10?
- What model provides the optimal performance?
- Apply this model to the test set. How does the test loss function compare to the training cross-validated RMSE?
