---
title: "bank_marketing_ex4"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Prereqs 

```{r slide-3}
# Packages required
library(rsample)
library(caret)
library(tidyverse)
library(inspectdf) 
library(ISLR)
library(dplyr)
library(ggplot2)
library(recipes)
library(psych)

# Model interpretability packages
library(vip)       # variable importance
library(ROCR)      # ROC curve

library(gbm)
library(xgboost)
library(pdp)
```



```{r}
#http://archive.ics.uci.edu/ml/datasets/Bank+Marketing

#read the data set
bank <- read.csv2("data/bank-additional-full.csv", dec = ".")
```

convert all characters to factors
```{r}
bank[sapply(bank, is.character)] <- lapply(bank[sapply(bank, is.character)], 
                                       as.factor)
```

drop duration and nr.employed columns 
```{r}
bank <- select(bank, c (-duration, - nr.employed))
```

convert all values in y into 0/1 in order to be accepted by xgboost
```{r}
bank$y <-  as.numeric(bank$y)-1
```

take smaller train set to check if the training method works well
```{r}
set.seed(123) # for reproducibility
split <- initial_split(bank, strata = "y", prop = 0.025)
train <- training(split)

```


#1
Apply a basic GBM model with the same features you used in the random forest module.

is true to apply 970 trees over all gbm and x and s??

how can I calculate the error?? the accuracy for classification ?
```{r slide-17}
set.seed(123)
bank_gbm <- gbm(
  formula = y ~ ., # unclass(y)-1 is to transform the number into {0,1} for bernoulli
  data = train,
  distribution = "bernoulli", # or bernoulli, multinomial, etc. 
  n.trees = 970, 
  shrinkage = 0.1, 
  interaction.depth = 1, 
  n.minobsinnode = 5, 
  cv.folds = 5 
  )  

# find index for n trees with minimum CV error
min_MSE <- which.min(bank_gbm$cv.error)
min_MSE
# get MSE and compute RMSE
bank_gbm$cv.error[min_MSE]
```

- Apply the default hyperparameter settings with a learning rate set to 0.10. 

```{r slide-17}
set.seed(123)
bank_gbm_10 <- gbm(
  formula = y ~ .,
  data = train,
  distribution = "bernoulli", # or bernoulli, multinomial, etc. 
  n.trees = 970, 
  shrinkage = 0.10, 
  interaction.depth = 1, 
  n.minobsinnode = 5, 
  cv.folds = 5 
  )  

# find index for n trees with minimum CV error
min_MSE <- which.min(bank_gbm_10$cv.error)

# get MSE and compute RMSE
#sqrt(bank_gbm_10$cv.error[min_MSE])

print(bank_gbm_10)

```
- How does model performance compare to the random forest module?

- How many trees were applied? 
970 trees

Use CV or OOB

the gbm package comes with a default function called gbm.perf() to determine the optimum number of iterations. 

we will also see two plots indicating the optimum number of trees based on the respective technique used. The graph on the left indicates the error on test (green line) and train data set (black line). The blue dotted line points the optimum number of iterations. One can also clearly observe that the beyond a certain a point (169 iterations for the “cv” method), the error on the test data appears to increase because of overfitting. Hence, our model will stop the training procedure on the given optimum number of iterations.

the optimum number of iterations is 48 based on "cv"
```{r slide-22}
gbm.perf(bank_gbm_10, method = "cv") # or "OOB"
```
- Was this enough to stabilize the loss function or do you need to add more?

- Tune the hyperparameters using the suggested tuning strategy for basic GBMs. 

# Tuning strategy 

1. fix tree hyperparameters
    - moderate tree depth
    - default min obs
2. set our learning rate at .01
3. increase CV to ensure unbiased error estimate
4. Results
   - Lowest error rate yet ($21,914.55)!
   - Used nearly all our trees --> increase to 6000?
   - took ~ 2.25 min
5. Compared to learning rate of .001
   - error rate of $24,791.66
   - took ~ 4 min

This model run takes ~2 mins 

```{r slide-28}
set.seed(123)
bank_gbm1 <- gbm(
  formula = y ~ .,
  data = train,
  distribution = "bernoulli", # or bernoulli, multinomial, etc. #<<
  n.trees = 970, #<<
  shrinkage = 0.01, #<<
  interaction.depth = 3, #<<
  n.minobsinnode = 10, #<<
  cv.folds = 10 #<<
  )

# find index for n trees with minimum CV error
min_MSE <- which.min(bank_gbm1$cv.error)

# get MSE and compute RMSE
#sqrt(bank_gbm1$cv.error[min_MSE])

gbm.perf(bank_gbm1, method = "cv")
```


-Did your model performance improve?


#2
Apply a stochastic GBM model. Tune the hyperparameters using the suggested tuning strategy for stochastic GBMs. 

# Applying a Stochastic GBM

- start by assessing if values between 0.5-0.8 outperform your previous best model
- zoom in with a second round of tuning
- smaller values will tell you that overfitting was occurring

```{r slide-32}
bag_frac <- c(.5, .65, .8) #<<

for(i in bag_frac) {
  set.seed(123)
  m <- gbm(
    formula = y ~ .,
    data = train,
    distribution = "bernoulli",
    n.trees = 970, 
    shrinkage = 0.01, 
    interaction.depth = 7, 
    n.minobsinnode = 5,
    bag.fraction = i, #<<
    cv.folds = 10 
    )
  # compute RMSE
  #print(sqrt(min(m$cv.error)))
  print(min(m$cv.error))
}
```

- Did your model performance improve?





#3
Apply an XGBoost model. Tune the hyperparameters using the suggested tuning strategy for XGBoost models.

# Prereqs

* __xgboost__ requires that our features are one-hot encoded
* __caret__ and __h2o::h2o.xgboost__ can automate this for you
* In this preprocessing I:
   - collapse low frequency levels to "other"
   - convert ordered factors to integers (aka label encode)
   
___Pro tip: If you have I cardinality categorical features, label or ordinal encoding often improves performance and speed!___

```{r slide-35}
library(recipes)
xgb_prep <- recipe(y ~ ., data = train) %>%
  step_other(all_nominal(), threshold = .005) %>%
  step_integer(all_nominal()) %>%
  prep(training = train, retain = TRUE) %>%
  juice()

X <- as.matrix(xgb_prep[setdiff(names(xgb_prep), "y")])
Y <- xgb_prep$y
```

# First XGBoost model 

* `nrounds`: 6,000 trees 
* `objective`: `reg:linear` for regression but other options exist (i.e. `reg:logistic`, `binary:logistic`, `num_class`)
* `early_stopping_rounds`: stop training if CV RMSE doesn't improve for 50 trees in a row 
* `nfold`: 10-fold CV 

___This grid search takes ~20 secs___

```{r slide-36}
set.seed(123)
bank_xgb <- xgb.cv(
  data = X,
  label = Y,
  nrounds = 970,
  objective = "binary:logistic",
  early_stopping_rounds = 50, 
  nfold = 10,
  verbose = 0,
  )  

bank_xgb$evaluation_log %>% tail()
```


# Tuning 

1. ___Crank up the trees and tune learning rate with early stopping___
   - initial test RMSE results:
   - .red[`eta = .3` (default): 24,382 w/200 trees (< 1 min)]
   - .red[`eta = .1`: 22,333 w/398 trees (< 1 min)]
   - .green[`eta = .05`: 21,877 w/978 trees (1.5 min)]
   - .red[`eta = .01`: 22,094 w/2843 trees (4 min)]
   
As a comparison, if you one-hot encoded the feature set it takes 30 mins to run with `eta = .01`!

___This grid search takes ~1.5 min___

How do we know that params = list(eta = .05) and what the results above ???
what is nrounds = 6000,???
```{r slide-37}
set.seed(123)
bank_xgb_eta <- xgb.cv(
  data = X,
  label = Y,
  nrounds = 970,
  objective = "binary:logistic",
  early_stopping_rounds = 50, 
  nfold = 10,
  verbose = 0,
  params = list(eta = .05) #<<
  )  

bank_xgb_eta$evaluation_log %>% tail()
```

# Tuning 

1. Crank up the trees and tune learning rate with early stopping
2. ___Tune tree-specific hyperparameters___
   - tree depth
   - instances required to make additional split

* Preferred values: 
   - `max_depth` = 3
   - `min_child_weight` = 3
   - RMSE = 20989.27

___This grid search takes ~30 min___


```{r slide-38}
# grid
hyper_grid <- expand.grid(
  eta = .05,
  max_depth = c(1, 3, 5, 7, 9), #<<
  min_child_weight = c(1, 3, 5, 7, 9), #<<
  error = 0 # a place to dump results
  )

# grid search
for(i in seq_len(nrow(hyper_grid))) {
  set.seed(123)
  m <- xgb.cv(
    data = X,
    label = Y,
    nrounds = 970,
    objective = "binary:logistic",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 0,
    params = list( #<<
      eta = hyper_grid$eta[i], #<<
      max_depth = hyper_grid$max_depth[i], #<<
      min_child_weight = hyper_grid$min_child_weight[i] #<<
    ) #<<
  )
  hyper_grid$error[i] <- min(m$evaluation_log$test_error_mean)
}

arrange(hyper_grid, error)
```

# Tuning 

1. Crank up the trees and tune learning rate with early stopping
2. Tune tree-specific hyperparameters
3. ___Add stochastic attributes with___
   - subsampling rows for each tree
   - subsampling columns for each tree 

* Preferred values: 
   - `subsample` = 0.80
   - `colsample_bytree` = 1
   - RMSE = 20732.22

___This grid search takes ~12 min___

you have to put max_depth and min_child_weight from the previous output
```{r slide-39}
# grid
hyper_grid <- expand.grid(
  eta = .05,
  max_depth = 3, 
  min_child_weight = 3,
  subsample = c(.5, .65, .8, 1), #<<
  colsample_bytree = c(.5, .65, .8, 1), #<<
  error = 0 # a place to dump results
  )

# grid search
for(i in seq_len(nrow(hyper_grid))) {
  set.seed(123)
  m <- xgb.cv(
    data = X,
    label = Y,
    nrounds = 970,
    objective = "binary:logistic",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 0,
    params = list( #<<
      eta = hyper_grid$eta[i],
      max_depth = hyper_grid$max_depth[i],
      min_child_weight = hyper_grid$min_child_weight[i],
      subsample = hyper_grid$subsample[i], #<<
      colsample_bytree = hyper_grid$colsample_bytree[i] #<<
    ) #<<
  )
  hyper_grid$error[i] <- min(m$evaluation_log$test_error_mean)
}

arrange(hyper_grid, error)
```


# Tuning

1. Crank up the trees and tune learning rate with early stopping
2. Tune tree-specific hyperparameters
3. Add stochastic attributes with
4. ___See if adding regularization helps___
   - gamma: tested 1, 100, 1000, 10000 -- no effect
   - lambda: tested 0.001, 0.01, 0.1, 1, 100, 1000 -- no effect
   - alpha: tested 0.001, 0.01, 0.1, 1, 100, 1000 -- minor improvement

* Preferred value:
   - `alpha` = 100
   - RMSE = 20581.31

___This grid search takes ~5 min___

```{r slide-41}
hyper_grid <- expand.grid(
  eta = .05,
  max_depth = 3, 
  min_child_weight = 3,
  subsample = .8, 
  colsample_bytree = 1,
  #gamma = c(1, 100, 1000, 10000),
  #lambda = c(1e-2, 0.1, 1, 100, 1000, 10000),
  alpha = c(1e-2, 0.1, 1, 100, 1000, 10000), #<<
  error = 0 # a place to dump results
  )

# grid search
for(i in seq_len(nrow(hyper_grid))) {
  set.seed(123)
  m <- xgb.cv(
    data = X,
    label = Y,
    nrounds = 970,
    objective = "binary:logistic",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 0,
    params = list( 
      eta = hyper_grid$eta[i], 
      max_depth = hyper_grid$max_depth[i],
      min_child_weight = hyper_grid$min_child_weight[i],
      subsample = hyper_grid$subsample[i], #<<
      colsample_bytree = hyper_grid$colsample_bytree[i],
      #gamma = hyper_grid$gamma[i], 
      #lambda = hyper_grid$lambda[i]#, 
      alpha = hyper_grid$alpha[i] #<<
    ) 
  )
  hyper_grid$error[i] <- min(m$evaluation_log$test_error_mean)
}

arrange(hyper_grid, error)
```

# Tuning 

1. Crank up the trees and tune learning rate with early stopping
2. Tune tree-specific hyperparameters
3. Add stochastic attributes with
4. See if adding regularization helps
5. If you find hyperparameter values that are substantially different from default settings, be sure to assess the learning rate again
6. ___Rerun final "optimal" model with `xgb.cv()` to get iterations required and then with `xgboost()` to produce final model___

___`final_cv`] test RMSE: 20,581.31___

```{r slide-42}
# parameter list
params <- list(
  eta = .05,
  max_depth = 3, 
  min_child_weight = 3,
  subsample = .8, 
  colsample_bytree = 1,
  alpha = 100
)

# final cv fit
set.seed(123)
final_cv <- xgb.cv(
  data = X,
  label = Y,
  nrounds = 970,
  objective = "binary:logistic",
  early_stopping_rounds = 50, 
  nfold = 10,
  verbose = 0,
  params = params #<<
  ) 

# train final model
bank_final_xgb <- xgboost(
  data = X,
  label = Y,
  nrounds = final_cv$best_iteration, #<<
  objective = "binary:logistic",
  params = params, #<<
  verbose = 0
)
```


- Did your model performance improve?

- Did regularization help?

#4
Pick your best GBM model. 

```{r slide-44}
vip::vip(bank_final_xgb, num_features = 25)
```

- Which 10 features are considered most influential? 

- Are these the same features that have been influential in previous models?




#5

Create partial dependence plots for the top two most influential features.
# Feature Effects (PDP)

```{r slide-45a}
bank_final_xgb %>%
  partial(
    pred.var = "pdays", 
    n.trees = bank_final_xgb$niter, 
    grid.resolution = 50, 
    train = X
    ) %>%
  autoplot(rug = TRUE, train = X)
```

# Feature Effects (ICE)

```{r slide-45b}
bank_final_xgb %>%
  partial(
    pred.var = "pdays", 
    n.trees = bank_final_xgb$niter, 
    grid.resolution = 50, 
    train = X,
    ice = TRUE
    ) %>%
  autoplot(rug = TRUE, train = X, alpha = .05, center = TRUE) 
```

- Explain the relationship between the feature and the predicted values.



