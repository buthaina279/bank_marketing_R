---
title: "bank_marketing_ex4"
author: "*Buthaina Alshareef*"
date: "*01-12-2020*"
output:
  html_document:
   toc: true
   toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




# Prereqs 

```{r slide-3, cache=TRUE}
# Packages required
library(rsample)
library(caret)
library(tidyverse)
library(inspectdf) 
library(ISLR)
library(dplyr)
library(ggplot2)
library(recipes)
library(psych)

# Model interpretability packages
library(vip)       # variable importance
library(ROCR)      # ROC curve

library(gbm)
library(xgboost)
library(pdp)
```



```{r, cache=TRUE}
#http://archive.ics.uci.edu/ml/datasets/Bank+Marketing

#read the data set
bank <- read.csv2("data/bank-additional-full.csv", dec = ".")
```

convert all characters to factors
```{r, cache=TRUE}
bank[sapply(bank, is.character)] <- lapply(bank[sapply(bank, is.character)], 
                                       as.factor)
```

drop duration and nr.employed columns 
```{r, cache=TRUE}
bank <- select(bank, c (-duration, - nr.employed))
```

convert all values in y into 0/1 in order to be accepted by xgboost
```{r, cache=TRUE}
bank$y <-  as.numeric(bank$y)-1
```

take smaller train set to check if the training method works well
```{r, cache=TRUE}
set.seed(123) # for reproducibility
split <- initial_split(bank, strata = "y", prop = 0.7)
train <- training(split)

```

```{r, cache=TRUE}
names(bank)
```

#1
Apply a basic GBM model with the same features you used in the random forest module.


```{r slide-18, cache=TRUE}
set.seed(123)
bank_gbm <- gbm(
  formula = y ~ ., 
  data = train,
  distribution = "bernoulli", # or bernoulli, multinomial, etc. 
  n.trees = 970, 
  shrinkage = 0.1, 
  interaction.depth = 1, 
  n.minobsinnode = 5, 
  cv.folds = 5 
  )  

# find index for n trees with minimum CV error
print(min(bank_gbm$cv.error))
```

- Apply the default hyperparameter settings with a learning rate set to 0.10. 

```{r slide-17, cache=TRUE}
set.seed(123)
bank_gbm_10 <- gbm(
  formula = y ~ .,
  data = train,
  distribution = "bernoulli", # or bernoulli, multinomial, etc. 
  n.trees = 5000, 
  shrinkage = 0.10, 
  interaction.depth = 1, 
  n.minobsinnode = 10, 
  cv.folds = 5 
  )  

# find index for n trees with minimum CV error
min_MSE <- which.min(bank_gbm_10$cv.error)


print(min(bank_gbm_10$cv.error))

```
- How does model performance compare to the random forest module?

the performance is better because when we increased the number of trees to 5000 and with 0.10 learning rate, the error down from 0.5492983 to 0.5461762


- How many trees were applied? 
5000 trees


we will also see two plots indicating the optimum number of trees based on the respective technique used. The graph on the left indicates the error on test (green line) and train data set (black line). The blue dotted line points the optimum number of iterations. One can also clearly observe that the beyond a certain a point (169 iterations for the “cv” method), the error on the test data appears to increase because of overfitting. Hence, our model will stop the training procedure on the given optimum number of iterations.

the optimum number of iterations is 4799 based on "cv"
```{r slide-22, cache=TRUE}
gbm.perf(bank_gbm_10, method = "cv") # or "OOB"
```
- Was this enough to stabilize the loss function or do you need to add more?
almost all the trees was used so we are going to increase the number of trees to be 6000 tree


- Tune the hyperparameters using the suggested tuning strategy for basic GBMs. 

# Tuning strategy 

1. fix tree hyperparameters
    - moderate tree depth
    - default min obs
2. set our learning rate at .01
3. increase CV to ensure unbiased error estimate

```{r slide-29, cache=TRUE}
# search grid
hyper_grid <- expand.grid(
  n.trees = 6000,
  shrinkage = .01,
  interaction.depth = c(3, 5, 7), #<<
  n.minobsinnode = c(5, 10, 15) #<<
)

model_fit <- function(n.trees, shrinkage, interaction.depth, n.minobsinnode) {
  set.seed(123)
  m <- gbm(
    formula = y ~ .,
    data = train,
    distribution = "bernoulli",
    n.trees = n.trees,
    shrinkage = shrinkage, #<<
    interaction.depth = interaction.depth, #<<
    n.minobsinnode = n.minobsinnode,
    cv.folds = 10
  )
  # compute RMSE
  min(m$cv.error)
}

hyper_grid$error <- pmap_dbl(
  hyper_grid,
  ~ model_fit(
    n.trees = ..1,
    shrinkage = ..2,
    interaction.depth = ..3,
    n.minobsinnode = ..4
    )
)

arrange(hyper_grid, error)
```

-Did your model performance improve?

yes, Lowest error rate yet (0.5393553)
Used all of our trees (6000)


#2
Apply a stochastic GBM model. 

Tune the hyperparameters using the suggested tuning strategy for stochastic GBMs. 

# Applying a Stochastic GBM

- start by assessing if values between 0.5-0.8 outperform your previous best model
- zoom in with a second round of tuning
- smaller values will tell you that overfitting was occurring

```{r slide-32, cache=TRUE}
bag_frac <- c(.5, .65, .8) #<<

for(i in bag_frac) {
  set.seed(123)
  m_bank <- gbm(
    formula = y ~ .,
    data = train,
    distribution = "bernoulli",
    n.trees = 6000, 
    shrinkage = 0.01, 
    interaction.depth = 7, 
    n.minobsinnode = 10,
    bag.fraction = i, #<<
    cv.folds = 10 
    )
 
  
  print(min(m_bank$cv.error))
}
```

- Did your model performance improve?
yes, Lowest error rate yet (0.5380045) 




#3
Apply an XGBoost model. 

Tune the hyperparameters using the suggested tuning strategy for XGBoost models.

# Prereqs

* __xgboost__ requires that our features are one-hot encoded
* __caret__ and __h2o::h2o.xgboost__ can automate this for you
* In this preprocessing I:
   - collapse low frequency levels to "other"
   - convert ordered factors to integers (aka label encode)
   
___Pro tip: If you have I cardinality categorical features, label or ordinal encoding often improves performance and speed!___

```{r slide-35, cache=TRUE}
library(recipes)
xgb_prep <- recipe(y ~ ., data = train) %>%
  step_other(all_nominal(), threshold = .005) %>%
  step_integer(all_nominal()) %>%
  prep(training = train, retain = TRUE) %>%
  juice()

X <- as.matrix(xgb_prep[setdiff(names(xgb_prep), "y")])
Y <- xgb_prep$y
```

# First XGBoost model 

* `nrounds`: 6,000 trees 
* `objective`: `binary:logistic` for classification 
* `early_stopping_rounds`: stop training if CV RMSE doesn't improve for 50 trees in a row 
* `nfold`: 10-fold CV 
the performance becomes 0.0989527


```{r slide-36, cache=TRUE}
set.seed(123)
bank_xgb <- xgb.cv(
  data = X,
  label = Y,
  nrounds = 6000,
  objective = "binary:logistic",
  early_stopping_rounds = 50, 
  nfold = 10,
  verbose = 0,
  )  

bank_xgb$evaluation_log %>% tail()
```


# Tuning 

1. ___Crank up the trees and tune learning rate with early stopping___
   
   `eta = .05`: 0.0975651 with 6000 trees 
  
```{r slide-37, cache=TRUE}
set.seed(123)
bank_xgb_eta <- xgb.cv(
  data = X,
  label = Y,
  nrounds = 6000,
  objective = "binary:logistic",
  early_stopping_rounds = 50, 
  nfold = 10,
  verbose = 0,
  params = list(eta = .05) #<<
  )  

bank_xgb_eta$evaluation_log %>% tail()
```

# Tuning 

1. Crank up the trees and tune learning rate with early stopping
2. ___Tune tree-specific hyperparameters___
   - tree depth
   - instances required to make additional split

* Preferred values: 
   - `max_depth` = 7
   - `min_child_weight` = 9
   - error = 0.0966287	

```{r slide-38, cache=TRUE}
# grid
hyper_grid <- expand.grid(
  eta = .05,
  max_depth = c(1, 3, 5, 7, 9), #<<
  min_child_weight = c(1, 3, 5, 7, 9), #<<
  error = 0 # a place to dump results
  )

# grid search
for(i in seq_len(nrow(hyper_grid))) {
  set.seed(123)
  m <- xgb.cv(
    data = X,
    label = Y,
    nrounds = 6000,
    objective = "binary:logistic",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 0,
    params = list( #<<
      eta = hyper_grid$eta[i], #<<
      max_depth = hyper_grid$max_depth[i], #<<
      min_child_weight = hyper_grid$min_child_weight[i] #<<
    ) #<<
  )
  hyper_grid$error[i] <- min(m$evaluation_log$test_error_mean)
}

arrange(hyper_grid, error)
```

# Tuning 

1. Crank up the trees and tune learning rate with early stopping
2. Tune tree-specific hyperparameters
3. ___Add stochastic attributes with___
   - subsampling rows for each tree
   - subsampling columns for each tree 

* Preferred values: 
   - `subsample` = 1.00	
   - `colsample_bytree` = 1.00	
   - error = 0.0966287

```{r slide-39, cache=TRUE}
# grid
hyper_grid <- expand.grid(
  eta = .05,
  max_depth = 7, 
  min_child_weight = 9,
  subsample = c(.5, .65, .8, 1), #<<
  colsample_bytree = c(.5, .65, .8, 1), #<<
  error = 0 # a place to dump results
  )

# grid search
for(i in seq_len(nrow(hyper_grid))) {
  set.seed(123)
  mm_bank <- xgb.cv(
    data = X,
    label = Y,
    nrounds = 6000,
    objective = "binary:logistic",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 0,
    params = list( #<<
      eta = hyper_grid$eta[i],
      max_depth = hyper_grid$max_depth[i],
      min_child_weight = hyper_grid$min_child_weight[i],
      subsample = hyper_grid$subsample[i], #<<
      colsample_bytree = hyper_grid$colsample_bytree[i] #<<
    ) #<<
  )
  hyper_grid$error[i] <- min(mm_bank$evaluation_log$test_error_mean)
}

arrange(hyper_grid, error)
```


# Tuning

1. Crank up the trees and tune learning rate with early stopping
2. Tune tree-specific hyperparameters
3. Add stochastic attributes with
4. ___See if adding regularization helps___
   - alpha: tested 0.001, 0.01, 0.1, 1, 100, 1000 -- minor improvement

* Preferred value:
   - `alpha` = 100
   - error = 0.0966633


```{r slide-41, cache=TRUE}
hyper_grid <- expand.grid(
  eta = .05,
  max_depth = 7, 
  min_child_weight = 9,
  subsample = 1., 
  colsample_bytree = 1,
  #gamma = c(1, 100, 1000, 10000),
  #lambda = c(1e-2, 0.1, 1, 100, 1000, 10000),
  alpha = c(1e-2, 0.1, 1, 100, 1000, 10000), #<<
  error = 0 # a place to dump results
  )

# grid search
for(i in seq_len(nrow(hyper_grid))) {
  set.seed(123)
  m_xgb <- xgb.cv(
    data = X,
    label = Y,
    nrounds = 6000,
    objective = "binary:logistic",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 0,
    params = list( 
      eta = hyper_grid$eta[i], 
      max_depth = hyper_grid$max_depth[i],
      min_child_weight = hyper_grid$min_child_weight[i],
      subsample = hyper_grid$subsample[i], #<<
      colsample_bytree = hyper_grid$colsample_bytree[i],
      #gamma = hyper_grid$gamma[i], 
      #lambda = hyper_grid$lambda[i]#, 
      alpha = hyper_grid$alpha[i] #<<
    ) 
  )
  hyper_grid$error[i] <- min(m_xgb$evaluation_log$test_error_mean)
}

arrange(hyper_grid, error)
```

# Tuning 

1. Crank up the trees and tune learning rate with early stopping
2. Tune tree-specific hyperparameters
3. Add stochastic attributes with
4. See if adding regularization helps
5. If you find hyperparameter values that are substantially different from default settings, be sure to assess the learning rate again
6. ___Rerun final "optimal" model with `xgb.cv()` to get iterations required and then with `xgboost()` to produce final model___

___`final_cv`] test error: 0.093299___

```{r slide-42, cache=TRUE}
# parameter list
params <- list(
  eta = .05,
  max_depth = 7, 
  min_child_weight = 9,
  subsample = 1, 
  colsample_bytree = 1,
  alpha = 1e-02
)

# final cv fit
set.seed(123)
final_cv <- xgb.cv(
  data = X,
  label = Y,
  nrounds = 6000,
  objective = "binary:logistic",
  early_stopping_rounds = 50, 
  nfold = 10,
  verbose = 0,
  params = params #<<
  ) 

# train final model
bank_final_xgb <- xgboost(
  data = X,
  label = Y,
  nrounds = final_cv$best_iteration, #<<
  objective = "binary:logistic",
  params = params, #<<
  verbose = 0
)

bank_final_xgb
```


- Did your model performance improve?
yes, error = 0.093299	
- Did regularization help?
yes 


#4
Pick your best GBM model. 

```{r slide-44, cache=TRUE}
vip::vip(bank_final_xgb, num_features = 25)
```

- Which 10 features are considered most influential? 

    1- euribor3m
    2- month
    3- pdays
    4- poutcome
    5- age
    6- day_of_week
    7- cons.price.idx
    8- contact
    9- campaign
    10- cons.conf.idx
    

- Are these the same features that have been influential in previous models?
no they are different

#5

Create partial dependence plots for the top two most influential features.
# Feature Effects (PDP)

```{r slide-45a, cache=TRUE}
bank_final_xgb %>%
  partial(
    pred.var = "euribor3m", 
    n.trees = bank_final_xgb$niter, 
    grid.resolution = 50, 
    train = X
    ) %>%
  autoplot(rug = TRUE, train = X)
```

# Feature Effects (ICE)

```{r slide-45b, cache=TRUE}
bank_final_xgb %>%
  partial(
    pred.var = "month", 
    n.trees = bank_final_xgb$niter, 
    grid.resolution = 50, 
    train = X,
    ice = TRUE
    ) %>%
  autoplot(rug = TRUE, train = X, alpha = .05, center = TRUE) 
```



```{r}
#save.image("myWorkspace_ex4.rds")
```


```{r}
load("myWorkspace_ex4.rds")
```


